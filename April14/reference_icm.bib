@book{brockman2020possible,
  title={Possible minds: Twenty-five ways of looking at AI},
  author={Brockman, John},
  year={2020},
  publisher={Penguin Books}
}


@incollection{collier1999causation,
  title={Causation is the transfer of information},
  author={Collier, John D},
  booktitle={Causation and laws of nature},
  pages={215--245},
  year={1999},
  publisher={Springer}
}

@article{VanderWeele2009,
abstract = {Cole and Frangakis (Epidemiology. 2009; 20: 3-5) introduced notation for the consistency assumption in causal inference. I extend this notation and propose a refinement of the consistency assumption that makes clear that the consistency statement, as ordinarily given, is in fact an assumption and not an axiom or definition. The refinement is also useful in showing that additional assumptions (referred to here as treatment-variation irrelevance assumptions), stronger than those given by Cole and Frangakis, are in fact necessary in articulating the ordinary assumptions of ignorability or exchangeability. The refinement furthermore sheds light on the distinction between intervention and choice in reasoning about causality. A distinction between the range of treatment variations for which potential outcomes can be defined and the range for which treatment comparisons arc made is discussed in relation to issues of nonadherence. The use of stochastic counterfactuals can help relax what is effectively being presupposed by the treatment-variation irrelevance assumption and the consistency assumption. Copyright {\textcopyright} 2009 by Lippincott Williams {\&} Wilkins.},
author = {{Vander Weele}, Tyler J.},
doi = {10.1097/EDE.0b013e3181bd5638},
issn = {10443983},
journal = {Epidemiology},
mendeley-groups = {info,ICM},
month = {nov},
number = {6},
pages = {880--883},
title = {{Concerning the consistency assumption in causal inference}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19829187},
volume = {20},
year = {2009}
}
@article{Richardson1986,
abstract = {The first system dynamics work did not include the use of causal-loop diagrams. Feedback structure was portrayed by equations or stock-and-flow diagrams. Such representations were natural for engineers. In an attempt to make system dynamics accessible to a wider range of people, causal-loop diagrams have become increasingly popular. In many texts and courses they are the first tool described. Indeed, recently several analysts have proposed that system dynamics studies can be carried out without the development of formal models at all (Morecroft 1985; Wolstenholme and Coyle 1983; Wolstenholme 1985). Causal-loop diagrams often figure prominently in such analyses. Yet even those who advocate the use of qualitative system dynamics are careful to point out that in all the successful applications of such qualitative methods the analysts have had extensive experience with formal model building. Nevertheless, it seems inevitable that people at all experience levels will continue to rely on causal-loop diagrams. In the following paper dating from 1976, George Richardson describes a variety of problems which often arise in causal-loop diagramming, both in the development of the diagrams and the explication of behavior from them. The main difficulties arise because causal-loop diagrams obscure the stock and flow structure of systems. We sometimes emphasize so heavily the role of feedback structure in generating behavior that the crucial role of accumulation processes is lost. Even experienced modelers are easily misled by causal-loop diagrams. I suggest the following experiment: take the causal-loop diagram for the family feud described in Richardson's paper and ask a random sample of system dynamics modelers or students how it will behave. In my experience, one will not only receive a wide range of answers but most of these will be incorrect. Then repeat the experiment using the stock-and-flow diagram (with a different group of people, obviously). While answers will still vary, the number of correct responses should rise. In recognition of these difficulties, there has been a revival of stock-and-flow diagrams as a means of communicating structure (Morecroft 1982). Richardson's paper should not be taken as an argument to abandon causal-loop diagrams or qualitative system dynamics, however. But it serves as a caution to the facile use of an easily abused technique. Despite their problems, causal-loop diagrams are likely to remain important tools for the communication of feedback structure.},
author = {Richardson, George P.},
doi = {10.1002/sdr.4260020207},
issn = {08837066},
journal = {System Dynamics Review},
mendeley-groups = {ICM},
number = {2},
pages = {158--170},
publisher = {Wiley},
title = {{Problems with causal-loop diagrams}},
url = {http://doi.wiley.com/10.1002/sdr.4260020207},
volume = {2},
year = {1986}
}
@article{Heyang2019,
abstract = {We proposed the info intervention, which intervening the information sending out from a node. We point out issues of existing definition of $\backslash$textit{\{}perfect intervention{\}} and claims that info intervention should be a substitute of it.},
archivePrefix = {arXiv},
arxivId = {1907.11090},
author = {Heyang, Gong and Ke, Zhu},
eprint = {1907.11090},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Heyang, Ke - 2019 - Info Intervention.pdf:pdf},
mendeley-groups = {必引用文章,SCM{\_}theory,do{\_}calculus,必引用文章/causalDG,ICM},
month = {jul},
title = {{Info Intervention}},
url = {http://arxiv.org/abs/1907.11090},
year = {2019}
}
@article{Everitt2019,
abstract = {Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.},
archivePrefix = {arXiv},
arxivId = {1902.09980},
author = {Everitt, Tom and Ortega, Pedro A. and Barnes, Elizabeth and Legg, Shane},
eprint = {1902.09980},
mendeley-groups = {ICM},
month = {feb},
title = {{Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings}},
url = {http://arxiv.org/abs/1902.09980},
year = {2019}
}
@article{Peters2020,
abstract = {A probabilistic model describes a system in its observational state. In many situations, however, we are interested in the system's response under interventions. The class of structural causal models provides a language that allows us to model the behaviour under interventions. It can been taken as a starting point to answer a plethora of causal questions, including the identification of causal effects or causal structure learning. In this chapter, we provide a natural and straight-forward extension of this concept to dynamical systems, focusing on continuous time models. In particular, we introduce two types of causal kinetic models that differ in how the randomness enters into the model: it may either be considered as observational noise or as systematic driving noise. In both cases, we define interventions and therefore provide a possible starting point for causal inference. In this sense, the book chapter provides more questions than answers. The focus of the proposed causal kinetic models lies on the dynamics themselves rather than corresponding stationary distributions, for example. We believe that this is beneficial when the aim is to model the full time evolution of the system and data are measured at different time points. Under this focus, it is natural to consider interventions in the differential equations themselves.},
archivePrefix = {arXiv},
arxivId = {2001.06208},
author = {Peters, Jonas and Bauer, Stefan and Pfister, Niklas},
eprint = {2001.06208},
mendeley-groups = {ICM},
month = {jan},
title = {{Causal models for dynamical systems}},
url = {http://arxiv.org/abs/2001.06208},
year = {2020}
}
@article{Lauritzen,
abstract = {Chain graphs are a natural generalization of directed acyclic graphs and undirected graphs. However, the apparent simplicity of chain graphs belies the subtlety of the conditional independence hypotheses that they represent. There are many simple and apparently plausible, but ultimately fallacious, interpretations of chain graphs that are often invoked, implicitly or explicitly. These interpretations also lead to ¯awed methods for applying background knowledge to model selection. We present a valid interpretation by showing how the distribution corresponding to a chain graph may be generated from the equilibrium distributions of dynamic models with feed-back. These dynamic interpretations lead to a simple theory of intervention, extending the theory developed for directed acyclic graphs. Finally, we contrast chain graph models under this interpretation with simultaneous equation models which have traditionally been used to model feed-back in econometrics.},
author = {Lauritzen, S. L. and Richardson, T. S.},
doi = {10.1111/1467-9868.t01-1-00340},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Dawid et al. - 2002 - Chain graph models and their causal interpretations ‐ Discussion on the paper by Lauritzen and Richardson.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {QA Mathematics},
mendeley-groups = {SCM{\_}theory,handbook{\_}of{\_}causal{\_}diagrams,ICM},
number = {3},
pages = {348--361},
title = {{Chain graph models and their causal interpretations}},
url = {http://www.di.unito.it/{~}radicion/tmp{\_}firb{\_}{\_}2009/chainGraphs.pdf http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.t01-1-00340/abstract},
volume = {64},
year = {2002}
}
@incollection{Spector2015,
abstract = {Causal influence diagrams (a.k.a causal loop diagrams) are one of the important tools used by system dynamists to model the dynamic feedback relationships among the components of a complex system. A system is said to be complex if (1) it has a large number of parts (or variables, agents, individuals, components); (2) there are many interrelationships or interactions among the parts of the system; (3) the parts produce combined effects (synergies) that are not easily foreseen and may often be novel or surprising (chaotic) to the observer; in other words, if there exists an emergent global dynamics resulting from the actions of its parts rather than being imposed by an external or a central agent. Complex systems are pervasive in our lives: the human nervous system, an ant colony, the climate system, ecosystems, economic systems such as the global economy, the Earth systems, epidemics (e.g., HIV, malaria), the stock market, energy market, meteorological systems, transportation systems, social systems (e.g., family groups, sports teams, legislative bodies), political parties, governments, supply-chain systems, management of multinational corporations, wars against insurgencies, and health-care legislations are all examples of complex systems. Due to their emergent behavior, complex systems cannot be understood by inspection or intuition; to deal with them intelligently one needs a set of tools to describe, analyze, and model them in order to be able to manipulate certain of their output parameters with minimum unintended consequences.},
author = {Spector, J. Michael},
booktitle = {The SAGE Encyclopedia of Educational Technology},
doi = {10.4135/9781483346397.n46},
mendeley-groups = {ICM},
title = {{Causal Influence Diagrams}},
year = {2015}
}
@misc{Kuang2020,
abstract = {Causal inference is a powerful modeling tool for explanatory analysis, which might enable current machine learning to become explainable. How to marry causal inference with machine learning to develop eXplainable Artificial Intelligence (XAI) algorithms is one of key steps towards to the artificial intelligence 2.0. With the aim of bringing knowledge of causal inference to scholars of machine learning and artificial intelligence, we invited researchers working on causal inference to write this survey from different aspects of causal inference. This survey includes the following sections: “Estimating average treatment effect: A brief review and beyond” from Dr. Kun Kuang, “Attribution problems in counterfactual inference” from Prof. Lian Li, “The Yule-Simpson paradox and the surrogate paradox” from Prof. Zhi Geng, “Causal potential theory” from Prof. Lei Xu, “Discovering causal information from observational data” from Prof. Kun Zhang, “Formal argumentation in causal reasoning and explanation” from Profs. Beishui Liao and Huaxin Huang, “Causal inference with complex experiments” from Prof. Peng Ding, “Instrumental variables and negative controls for observational studies” from Prof. Wang Miao, and “Causal inference with interference” from Dr. Zhichao Jiang.},
author = {Kuang, Kun and Li, Lian and Geng, Zhi and Xu, Lei and Zhang, Kun and Liao, Beishui and Huang, Huaxin and Ding, Peng and Miao, Wang and Jiang, Zhichao},
booktitle = {Engineering},
doi = {10.1016/j.eng.2019.08.016},
issn = {20958099},
keywords = {Causal discovery,Causal inference,Causal reasoning and explanation,Counterfactual inference,Instructive variables,Negative control,Treatment effect estimation},
mendeley-groups = {ICM},
month = {jan},
publisher = {Elsevier Ltd},
title = {{Causal Inference}},
year = {2020}
}
@article{Everitt2019a,
abstract = {Proposals for safe AGI systems are typically made at the level of frameworks, specifying how the components of the proposed system should be trained and interact with each other. In this paper, we model and compare the most promising AGI safety frameworks using causal influence diagrams. The diagrams show the optimization objective and causal assumptions of the framework. The unified representation permits easy comparison of frameworks and their assumptions. We hope that the diagrams will serve as an accessible and visual introduction to the main AGI safety frameworks.},
archivePrefix = {arXiv},
arxivId = {1906.08663},
author = {Everitt, Tom and Kumar, Ramana and Krakovna, Victoria and Legg, Shane},
eprint = {1906.08663},
journal = {CEUR Workshop Proceedings},
mendeley-groups = {ICM},
month = {jun},
publisher = {CEUR-WS},
title = {{Modeling AGI Safety Frameworks with Causal Influence Diagrams}},
url = {http://arxiv.org/abs/1906.08663},
volume = {2419},
year = {2019}
}
@article{Everitt2019b,
abstract = {Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction using causal influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes can the agent have an incentivize to observe, and (2) which nodes can the agent have an incentivize to control? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different causal influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.},
archivePrefix = {arXiv},
arxivId = {1902.09980},
author = {Everitt, Tom and Ortega, Pedro A. and Barnes, Elizabeth and Legg, Shane},
eprint = {1902.09980},
mendeley-groups = {ICM},
month = {feb},
title = {{Understanding Agent Incentives using Causal Influence Diagrams. Part I: Single Action Settings}},
url = {http://arxiv.org/abs/1902.09980},
year = {2019}
}
@book{BernardFeltz2019,
author = {{Bernard Feltz}, Marcus Missal and Andrew Cameron Sims},
booktitle = {Free Will, Causality, and Neuroscience},
doi = {10.1163/9789004409965},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Bernard Feltz - 2019 - Free Will, Causality, and Neuroscience.pdf:pdf},
mendeley-groups = {ICM},
month = {aug},
publisher = {Brill | Rodopi},
title = {{Free Will, Causality, and Neuroscience}},
year = {2019}
}
@article{Pearl2019seven,
abstract = {THE DRAMATIC SUCCESS In machine learning has led to an explosion of artificial intelligence (AI) applications and increasing expectations for autonomous systems that exhibit human-level intelligence. These expectations have, however, met with fundamental obstacles that cut across many application areas.},
author = {Pearl, Judea},
doi = {10.1145/3241036},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Pearl - 2019 - The seven tools of causal inference, with reflections on machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
mendeley-groups = {必引用文章,必引用文章/causalDG,causal{\_}general,info,ICM},
number = {3},
pages = {54--60},
title = {{The seven tools of causal inference, with reflections on machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=3314328.3241036},
volume = {62},
year = {2019}
}
@book{Pearl2018,
abstract = {First edition. "May 2018"--Title page verso. "Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world"-- "Correlation is not causation"--this was one of the standards of scientific belief for a century. Now Pearl and his colleagues establish causality--the study of cause and effect--on a firm scientific basis. Causality doesn't just enable us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It is not just a victory for common sense, but a revolution in the study of the world.--adapted from dust jacket. Introduction : Mind over data -- The ladder of causation -- From buccaneers to guinea pigs : the genesis of causal inference -- From evidence to causes : Reverend Bayes meets Mr. Holmes -- Confounding and deconfounding : or, slaying the lurking variable -- The smoke-filled debate : clearing the air -- Paradoxes galore! -- Beyond adjustment : the conquest of Mount Intervention -- Counterfactuals : mining worlds that could have been -- Mediation : the search for a mechanism -- Big data, artificial intelligence, and the big questions.},
annote = {For many researchers, the most (perhaps only) familiar method of predicting the effect of an intervention is to “control” for confounders using the adjustment formula. This is the method to use if you are confident that you have data on a sufficient set of variables (called deconfounders) to block all the back-door paths between the intervention and the outcome. To},
author = {Pearl, Judea and Mackenzie, Dana and Book, Hachette and Books, Basic and Book, Hachette and York, New and Edition, First and Books, Basic and Books, Perseus and Group, Hachette Book and Books, The Basic and Group, Hachette Book and Hachette, The and Bureau, Speakers and Catalogingpublication, Congress and Names, Data and Pearl, Judea and Mackenzie, Dana and York, New and Books, Basic},
booktitle = {Science},
doi = {10.1126/science.aau9731},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Pearl et al. - 2018 - The Book of Why The New Science of Cause and Effect.pdf:pdf;:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Pearl et al. - 2018 - The Book of Why The New Science of Cause and Effect(2).pdf:pdf},
isbn = {9780465097616},
issn = {0036-8075},
mendeley-groups = {必引用文章,必引用文章/causalDG,causal{\_}books,handbook{\_}of{\_}causal{\_}diagrams,info,ICM},
title = {{The Book of Why: The New Science of Cause and Effect}},
year = {2018}
}
@article{Bongers2016,
abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
archivePrefix = {arXiv},
arxivId = {1911.10500},
author = {Sch{\"{o}}lkopf, Bernhard},
eprint = {1911.10500},
file = {:Users/gong/Library/Application Support/Mendeley Desktop/Downloaded/Bongers et al. - 2016 - Theoretical Aspects of Cyclic Structural Causal Models.pdf:pdf},
mendeley-groups = {必引用文章,do{\_}calculus,必引用文章/causalDG,handbook{\_}of{\_}causal{\_}diagrams,info,ICM},
month = {nov},
title = {{Causality for Machine Learning}},
url = {http://arxiv.org/abs/1611.06221 http://arxiv.org/abs/1911.10500},
year = {2019}
}
