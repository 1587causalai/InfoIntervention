\section{Info Intervention} 
\label{sec:info}


 
% 需要更多的文献支持
% 从从例子出发，给出定义，然后哲学讨论。


% In general, we need to answer questions such as what if I make something happpen.

Causal questions, such as what if we make something happen, can be formalized by $do$ intervention, which, however, is still controversial on empirical understanding as 
mentioned by Pearl \cite{Pearl2019do, pearl2018does}.
In many settings, a \emph{do} intervention which forces some variable to a given value is somewhat idealized or hypothetical. 
For instance, it is controversial to manipulate variables such as age, race, obesity, or cholesterol level by setting their values to some hypothetical ones in \emph{do} intervention. To see this point more clearly, we consider the following real example. 
\begin{Eg} %[A real example ``reference???'']
    \label{eg:real}
	Assume a causal relationship among 6 domain variables in Figure 1.
	%from which we can study the causal effect of Exercise on Cholesterol level.  
	Suppose that we aim to study the causal effect of Age on Income  by do intervention. 
	However, the empirical interpretation of the intervention $do(Age = \tilde{a})$ 
	is controversial for a hypothetical positive integer $\tilde{a}$,  though Pearl suggested that we should interpret it in other dimensions \cite{Pearl2019do}. In other words, it seems unreasonable to articulate a causal question that what if the \emph{Age} equals $\tilde{a}$ under do intervention framework, indicating that do intervention is not complete for formalizing real world causal queries. 
    \begin{figure}[ht]
        \label{fig:real}
    	\centering
   	\begin{tikzpicture}
    	\node[state] (O) at (0.0, 2.0) {$Occupation$};
    	\node[state] (d) at (0,0) {$Exercise$};
     	\node[state] (I) at (2.9, 2.0) {$Income$};
    	\node[state] (x) at (2.3,0.8) {$Age$};
    	\node[state] (D) at (4.5, 1.1) {$Diet$};
    	\node[state] (y) at (4.5,0) {$Cholesterol$};
    	\path (x) edge (d);
    	\path (x) edge (y);
    	\path (d) edge (y);
    	\path (O) edge (d);
    	\path (x) edge (I);
    	\path (O) edge (I);
    	\path (I) edge (D);
    	\path (D) edge (y);
    	\end{tikzpicture}
    	\caption{A causal relationship}
    	\label{fig:exercise}
    \end{figure}
\end{Eg}

Although we can not force \emph{Age} to $\tilde{a}$ in Example \ref{eg:real}, we can send out the information that \emph{Age} is $\tilde{a}$. This motivates us to consider a new causal semantic based on \emph{info} intervention operator, whose formal definition is given below. 

\begin{Def}[\emph{Info} intervention]
	\label{def:i-intervention}
	Given an \SCM\,$M=(G^+,\Xcal,\Pr, f)$ and any $I \subseteq V$, the info intervention $\info(X_I=x_I)$ (or, in short, $\info(x_I)$) maps $M$ to the intervened model $M_{\sigma(X_I = x_I)} = (G^+,\Xcal,\Pr, \tilde{f})$, 
	where $\tilde{f}_i := f_i(X_U, \tilde{X}_V)$ with
	$$
	\begin{aligned}
	\tilde{X}_j := \begin{cases}
	x_j, & j \in I, \\
	X_j, & j \in V \setminus I \,.
	\end{cases}
	\end{aligned}
	$$
\end{Def}

Based on the preceding definition, 	
the \emph{info} intervened \SCM\,$M_{\info(X_I=x_I)}$ 
does not delete any function from the model, but just sends out the information 
$X_I = x_I$ to the model. Since the information $X_I=x_I$ is received by 
the Children nodes of $X_I$, the links between $X_I$ and its Children nodes 
are removed in $M_{\info(X_I=x_I)}$.

Next, we highlight that the critical difference of 
 \emph{info} intervened \SCM\,$M_{\info(X_I=x_I)}$ and 
\emph{do} intervened \SCM\,$M_{do(X_I=x_I)}$ is that the former  
keeps the causal mechanisms (or structural equations $f_{V}$ in other words) unchanged while the latter doesn't. 
This difference makes 
the \emph{info} intervention have no non-manipulable variables problem, which, however, exists 
in \emph{do} intervention. For example, we now can articulate 
the causal question that what if the \emph{Age} equals $\tilde{a}$ in Example \ref{eg:real} under \emph{info} intervention framework, since $M_{\info(X_I=x_I)}$ receives the hypothetical value of \emph{Age}, and at the same time, it does not change the value of this invented variable \emph{Age}. 


To further illustrate how the \emph{info} intervention works and what is the difference between \emph{info} and \emph{do} interventions, we consider the following example: 



% 箭头代表有方向，等于符号代表没有方向。

\begin{Eg}
	\label{eg1}
	An \SCM\,with  
	a treatment $T$, a confounder $X$, an outcome $Y$, 
	and two latent variables $\epsilon_X, \epsilon_T$ is given in Figure \ref{fig:info-VS-$do$}(a), and its structural equations are:
	$$
	\begin{aligned}
	\begin{cases}
	X \leftarrow f_X(\epsilon_X),  \\
	T \leftarrow f_T(\epsilon_T, X), \\
	Y \leftarrow f_Y(X, T).
	\end{cases}
	\end{aligned}
	$$    
	Based on Definition \ref{def:p-intervention}, its 
	do intervened \SCM\,given in Figure \ref{fig:info-VS-$do$}(b) has the following structural equations: 
	$$
	\begin{aligned}
	\begin{cases}
	X \leftarrow f_X(\epsilon_X),  \\
	T \leftarrow t, \\
	Y \leftarrow f_Y(X, T).
	\end{cases}
	\end{aligned}
	$$    	
	Based on Definition \ref{def:i-intervention}, its info intervened \SCM\,given in Figure \ref{fig:info-VS-$do$}(c) has the following structural equations:
	$$
	\begin{aligned}
	\begin{cases}
	X \leftarrow f_X(\epsilon_X),  \\
	T \leftarrow f_T(\epsilon_T, X), \\
	Y \leftarrow f_Y(X, t). 
	\end{cases}
	\end{aligned}
	$$ 
As expected, the causal mechanisms are unchanged only in the info intervened \SCM\,.
\end{Eg}

\begin{figure}[ht]
	\begin{subfigure}[tb]{1.0\textwidth}
		\centering
		\begin{tikzpicture}
		
		\node[state] (d) at (0,0) {$T$};
		\node[state] (ed) at (-0.5, 1) {$\epsilon_T$};
		\node[state] (x) at (0.5, 1.5) {$X$};
		\node[state] (ex) at (2.2,1.5) {$\epsilon_X$};
		\node[state] (y) at (2,0) {$Y$};
		\path (ex) edge (x);
		\path (x) edge (d);
		\path (ex) edge (x);
		\path (ed) edge (d);
		\path (d) edge (y);
		\path (x) edge (y);
		\end{tikzpicture}
		\caption{An \SCM\,without intervention.}
		\label{fig:SCM1}
	\end{subfigure}
	\vfill
	\begin{subfigure}[tb]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
		\node[state] (d) at (0,0) {$T$};
		\node[state] (x) at (0.5,1.5) {$X$};
		\node[state] (y) at (2, 0) {$Y$};
		\node[state] (ex) at (2.0,1.5) {$\epsilon_X$};
		\node[state] (ed) at (-0.5, 1) {$\epsilon_T$};
		\path (ex) edge (x);
		\path (x) edge (y);
		\path (d) edge (y);
		\end{tikzpicture}
		\caption{Its \emph{do} intervened \SCM.}
		\label{fig:SCM2}
	\end{subfigure}
	\begin{subfigure}[tb]{0.5\textwidth}
		\centering
		\begin{tikzpicture}
		\node[state] (d) at (0,0) {$T$};
		\node[state] (x) at (0.5,1.5) {$X$};
%		\node[state, scale=0.5] (t) at (0.7, 0) {$T=t$};
		\node[state] (y) at (2, 0) {$Y$};
		\node[state] (ex) at (2.0,1.5) {$\epsilon_X$};
		\node[state] (ed) at (-0.5, 1) {$\epsilon_T$};
%		\path (t) edge (y) ;
		\path (ex) edge (x);
		\path (ed) edge (d);	
		\path (x) edge (y);
		\path (x) edge (d);
		\end{tikzpicture}
		\caption{Its \emph{info} intervened \SCM.}
		\label{fig:SCM3}
	\end{subfigure}
	\caption{An \SCM\,and its two intervened \SCM$\text{s}$.}
	\label{fig:info-VS-$do$}
\end{figure}

Based on our \emph{info} intervention, 
a new way to study causality can be 
implemented. In this \emph{info} intervention framework, we essentially treat the causality as the \emph{information transfer}. That is, we say a node $T$ causes another node $Y$, 
if the information of $T$ transferred by its output edges has an effect on $Y$. 
Our new viewpoint of causality is different from that of 
``Law-like'' causality in  physics or 
interventionist causality in statistics, economics and social sciences. 
To some extents, our information transfer causality seems to be the combination of these two, since the causal mechanisms are unchanged as in 
``Law-like'' causality, while the intervention operations are allowed as in 
interventionist causality. Particularly, in our information transfer causality framework, we say that $T$ causes $Y$ iff 
``\emph{changing information on $T$ leads to a change in $Y$, while keeping everything else constant}''. This viewpoint is different from that in the interventionist causality framework, under which one says that $T$ causes $Y$ iff 
``changing $T$ leads to a change in $Y$, while keeping everything else constant''.
We think our new viewpoint of causality based on information transfer could be beneficial for many applications, since 
as demonstrated before, it allows us to 
formalize, process and understand more causal relationships in the real world.  







% 需要说清楚，我们关注的点是干预后的分布，而不仅仅是 $Y$ 的分布。

% 有些必须用 $do$ intervention，比如RCT用药与否，有些必须用 info intervention 比如年龄对收入的影响。举例说明有些问题适合 do intervention, 有些适合用 info intervention. 


%
%\section{Information Processing view of Causal Modeling}
%
% 
%A model is an idealized representation of reality that highlights some aspects and ignores others. The info intervention semantic is to highlight the information processing aspect of causal model. 
%
%
%\paragraph{The phyical nature of information.} In Principle \ref{Prin:info}, causal models are considered as systems which process information. The way of how to generate and convert forms of energy has lead to the first and second industrial revolution, while information plays the same role of energy in the recent AI revolution\cite{Scholkopf2019}. If we think it broadly, every physical system is about processing information even physical are information-theoretic in origin \footnote{It from bit symbolizes the idea that every itme of the physical world has at bottom...an immaterial source and explanation...that all things physical are information-theoretic in origin and that this is a participatory universe. ---- John A. Wheeler}. One of the most intriguing problems would be that the conservation of information might also be a consequence  of symmetries \footnote{Emmy Noether claims that energy conservation is due to a symmetry of the fundamental laws of physics: they look the same no matter how we shift the time, in present, past, and future. Einsten was relying on covariance principles when he established the equivalence between energy and mass.}.
%
%Modeling physical phenomena with a set of coupled differrential equations is a gold standard. It allows us to predict the future behavior of a system, to reason about the effect of interventions in the stystem, and predict statistical dependences\cite{Scholkopf2019}. But such models requires an intelligent human to come up with it thus restrict its usage only in certain applications. Statistical models can be viewd as a much more superficial one are widely used in almost every scientific area with data, but very limited explainability and unable to answer causal question. Causal modeling lies in between these two extremes. It's true that we human can live well  without knowing causal mechansims of a liver, but what should we do when your liver goes wrong? Causal models should be convinient approximation of physical reality which could be help us fix problems when things go wrong, thus consistent with current scientific understanding of physical world matters, especially for building true intelligent systems.
%
%
%\paragraph{Control VS Interfere.} In the control system theory, say, a typical PID control systems, uses a feedback controller to make the system output to be a certain value. The widely used control system reminds us $do$ intervention which forces some variables take given values should interpreted with a feedback control system, thus $do$ intervention is more of controlling the state of a system with a sequence of control input signals. In contrast, our info intervention is just interfering the output information of the system at one particular timestap, which reflects the local property of causality. The issues of empirical understanding of $do$-intervention on non-manipulable variables do not exist for info intervention.
%
%When it comes to causality, there will alway a temporal or order structure underlying the causal model which is the essential difference comparing to statistical models. For directed acyclic graphs, the temporal structure is implied by the second statement of Principle \ref{Prin:info}. But for feedback loops which shown up in extensive kinds of real-world problems and complex systems, the temporal sturcture is messed up. The information view of causal models can be help in this situation. In real-world situations, it spends some time for a node that accepts, processes, and sends information while the variables in feedback loops are usually measured at the same give timestap. This inconsistency between data and causal mechanisms suggest that state of a node could be affected by historical information of its cause nodes.
%
%\paragraph{Info Markov property.} Causal diagrams perform as parsimonious representation to encode causal knowledges transparently, access swifty and do it with ease. Conditional independence structure among variables plays a central role which induces different kind of Markov properties corresponding to a form of factorization. 
%
%\begin{Def}[Info Markov property]
%	For an info causal model $G$, every  endogenous variable $X$ is  indenpent of its  non-descents given informations from their parents, then we $G$ satistfies info Markov property.   
%\end{Def}
%
%For the causal DAGs, this info markov property is the same with causal Markov property, but when it comes to causal graphs with feedback loops where nodes may accept historical input informations, these two Markov property can be different.

%In others words, uncertainties about the information processing progress exist. 




%The property is basically saying, endogenous variables in $V$ are locally dependent on other variables. The requirement of independence on $U$ in a SCM serves as a counterpart to info markov property in ICM. 

%An causal mechanism represented by a node $X$ in ICM is essentially stochastic. In particularly, $X \sim f_X(x|x^{in})$  given that all informations $X^{in} = x^{in}$ on its input edges are collected, and automatically sends information $X = x^{out}$ sampled from $f_X(x|x^{in})$ to its output edges. This local causal mechanism is not only invariant but also indepedent of others which is a fundamental building blocks of ICM. If we look into Example \ref{eg:info_causal_DAG}, an info intervention $\info(A=a^*, B=b^*)$ is implemented on the model. Then informations on output edges of $A$ and $B$  would change to $A=a^*, B=b^*$, while leaving the rest of the model intact.  


%There many kinds of graphs models, satisfy corresponding local markov properties which suggest some form of factorization. The most commonly used kind of graphs is DAG, we have already implemented info view of causal semantic on it which is similar to casual DAGs. When it comes to the situation of cyclic directed graphs, it is very straightforward to interpret causal semantics but many technique complications occur\cite{Bongers2016}. Researcher are keep searching for graph model, such as chain graphs, PDAG, while many totally opposite opinions  exist now. 
%
%
%Our information view of the causal model of extended world is shown in Figure \ref{fig:infoscm} that data information consist of enviroment information and mechanism information, which is similar to SCM framework used in \cite{arjovsky2019invariant}, and it can be dated back to the concept of signal-flow graph in the 1950s while many controversials on causal interpretation of it. There are many details about this information view of causal models, we start from the simplest case of DAGs.
%
%
%\begin{figure}[h]
%	\centering
%	\begin{tikzpicture}[sibling distance=10em,
%	every node/.style = {shape=ellipse, rounded corners,
%		draw, align=center,
%		top color=white, bottom color=blue!20}]]
%	
%	\node {Data information}
%	child { node {Environment \\ information} }
%	child { node {Mechanism\\ information}
%		child { node {Exogenous} }
%		child { node {Endogenous} } };
%	\end{tikzpicture}
%	\caption{An event  or record includes informations from the environment and causal mechanisms. }
%	\label{fig:infoscm}	
%\end{figure}
%

% For the simplest case that no causal relathionship among visible variable, see Remark \ref{rem1}.  An $do$ intervened SCM would be the same as the original SCM except for those intervened variables. For info intervention, the intervened SCM would exactly the same as the original SCM but the informations sending out from intervened variables change to the intervened values. 


\section{\emph{Info}-causal DAGs}

Analogous to the causal DAGs in the framework of 
\emph{do} intervention, it is natural to introduce our \emph{info}-causal DAGs below, under which the causal semantics  could be well defined without any complications in the framework of \emph{info} intervention. 



\begin{Def}[\emph{Info}-causal DAG]
	Consider a DAG $G = (V, E)$ and a random vector $X = (X_1, ..., X_K)$ with distribution $p$. Then, $G$ is called an info-causal DAG for $X$ if $p$ satisfies the following:
	\begin{enumerate}[(i)]
		\setlength{\itemsep}{0pt}
		\item $p$ factorizes, and thus is Markov, according to $G$,  
		\item for any $A \subseteq V$ and any $\tilde{x}_A$ in the domains of $X_A$,
		\begin{equation}\label{eq:sigma}
			p(x| \sigma(\tilde{x}_A)) = \prod_{k \in V} p(x_k|x_{pa(k)}^*),
		\end{equation}		
		where $x^*_k = x_k$ if $k \notin A$ else $\tilde{x}_k$.
	\end{enumerate}
\end{Def}

If $G$ is an \emph{info}-causal DAG, we can define its \emph{info}-intervened DAG.


\begin{Def}[\emph{Info}-intervened DAG]
	Consider an info-causal DAG $G = (V, E)$ and an info intervention $\sigma(\tilde{x}_A)$. The info-intervened DAG, denoted by $G^{\sigma(\tilde{x}_A)}$, has the joint distribution $p(x| \sigma(\tilde{x}_A))$ in (\ref{eq:sigma}) for the 
	counterfactual random vector $X^{\sigma(\tilde{x}_A)}$. 
\end{Def}

Due to the distinct way of intervention,  
the \emph{info}-intervened DAG removes all output edges of intervened nodes, while the \emph{do}-intervened DAG removes all input edges of intervened nodes; consequently, 
the distribution of $X^{\sigma(\tilde{x}_A)}$ in \emph{info}-causal DAG is different that of 
$X^{do(\tilde{x}_A)}$ in causal DAG. Although our 
\emph{info}-causal (or \emph{info}-intervened) DAG is different from the causal (or \emph{do}-intervened) DAG, 
they share many similar properties as demonstrated in the sequel. Below, we first introduce one of them.



% The causal graph of intervened SCM is defined as removing all input edges of those corresponding intervened variables. In contrast, intervened causal graph can be induced by removing all output edges for info intervention.


\begin{Lem}
	\label{Lem:consistent}
	For arbitrary disjoint sets  $S$ and $T$ of an info-causal DAG $G=(V,E)$,  
	$$
	p(x_S|x_T, \info(x_T)) = p(x_S|x_T).
	$$
\end{Lem}

\begin{proof}
	Taking $\tilde{x}_{A}=x_{T}$ in (\ref{eq:sigma}) , we have $
	p(x|\info(x_T)) = \prod_{k\in V} p(x_k|x_{pa(k)})=p(x).
	$
	By the marginalization on $S \cup T$ and $S$, it follows that 
	\begin{align}
	p(x_S, {x}_T|\info({x}_T)) &= p(x_S, {x}_T),\label{a1} \\
	p({x}_T|\info({x}_T)) &= p({x}_T).\label{a2}
	\end{align}
   Then, the result holds by the above two equations.
\end{proof}

The preceding lemma is analogous to the widely used consistency assumption\cite{VanderWeele2009} in the potential outcome framework, which states
$P(Y^{\tilde{a}} = y | A=\tilde{a}) = P(Y=y|A=\tilde{a})$.



% SWIGs\cite{richardson2013single} removing constant splited nodes. .

% 信息干预做的事情是 $do$ intervention, 我们的信息干预模型联合了两个框架

In the causal DAG, Pearl raised the concept of \emph{three-level causal hierarchy} to articulate the causal questions into three levels: 1. Association; 2. Intervention; 3. Counterfactuals \cite{Pearl2019seven}. 
This classification of causal questions gives us a useful insight on what kind of questions each class is capable of answering, and questions at level $i$ can be answered only if information from level $j$ $(>i)$ is available.
Similar to Pearl's idea, we can give a  three-level causal hierarchy in the \emph{info}-causal DAG:
\begin{enumerate}[1.]
	\setlength{\itemsep}{-1pt}
	\item Association $p(y|x)$
	\begin{itemize}
		\setlength{\itemsep}{0pt}		
		\item Typical activity: Seeing
		\item Typical questions: What is? How would seeing $X$ change my belief in $Y$?
		\item Examples: What does the habit of exercise information tell me about the cholesterol level? 
	\end{itemize}
	\item Intervention $p(y|\sigma(x), z)$
	\begin{itemize}
		\setlength{\itemsep}{0pt}		
		\item Typical activity: Intervening
		\item Typical questions: What if?  What if I manipulate the information sending out from $X$?
		\item Examples: What will the income be if I told the company the information that my age is 32?
	\end{itemize}
	\item Counterfactuals $p(y^{\sigma(x) }|x', y')$
	\begin{itemize}
		\setlength{\itemsep}{0pt}		
		\item Typical activity: Imagining, Retrospection
		\item Typical questions: Why?  Was it the information of $X$ that caused $Y$? 
		\item Examples: Was it the information of young age caused me to have low income?  What if I told the company the information that my age is 32? 
	\end{itemize}
\end{enumerate}

Both Pearl's and our 
three-level causal hierarchies can formulate a large range of causal questions. An advantage of our three-level causal hierarchy is that the causal questions for non-manipulable variables are not controversial. 
This is probably due to the fact that the \emph{info} intervention does not change the causal mechanisms of the DAG but just sends send out the intervention information to the DAG. 
To make this point more clearly, we re-consider Example \ref{eg:real}. In this example, the intervening question that what if I force or manipulate my age to 32,  i.e., $P(\mathit{Income}|do(\mathit{Age}=32))$, is controversial. In contrast, the intervening question $P(\mathit{Income}| \sigma(\mathit{Age}=32))$ can be interpreted as what will happen to my income if the company received the message that my age is 32, and its further counterfactual or retrospective question $P(\mathit{Income} | {\sigma(\mathit{Age}=32)}, \mathit{Income}=low, \mathit{Age}=22))$  can be interpreted as what if the company accepted the information that may age is 32 given that my 
true age is 22 and my true income is low.




 %a complementary to formalize causal questions. 





% The information on a specific edge is usually as the state of its pointing node, but an info intervention on the causal model may change informations to some events.



\section{Causal Calculus for Info Intervention}


The \emph{do} intervention is a standard for studying the causality, since it serves (at least) two purposes: communication and theoretical focus\cite{Pearl2009}. 
In the previous sections, we have already shown that our \emph{info} intervention is also a nice standard for communicating about causal questions. 
In this section, we will show our \emph{info} intervention can also 
be a good standard for theoretically focusing on the causal inference. That is, 
the theoretical results established for \emph{info} intervention are applicable to 
many variants of causal queries.


First, we give an adjustment formula for \emph{info} intervention, which is valid for the most common treatment-outcome case without confounding variables.
 
 \begin{Thm}[Adjustment formula]
 	For an info-causal DAG $G=(V,E)$, $T$ is the treatment, $X$ is the covariate, and $Y$ is the outcome. Then,
 	$$
 	p(y|\info(\tilde{t})) = \sum_x p(y|\tilde{t}, x) p(x).
	$$
 \end{Thm}
\begin{proof}
	By (\ref{eq:sigma}), 
 	$p(y, t, x|\sigma(\tilde{t})) = p(y|\tilde{t}, x) p(t|x) p(x)$. 
 	Then, it follows that
 	\begin{align*}
 		p(y|\info(\tilde{t})) &= \sum_x \sum_t p(y, t, x|\sigma(\tilde{t})) \\
 		&= \sum_x \sum_t p(y|\tilde{t}, x) p(t|x) p(x) \\
 		&= \sum_x p(y|\tilde{t}, x) p(x) \sum_t p(t|x) \\
 		&= \sum_x p(y|\tilde{t}, x) p(x).
 	\end{align*}
 	This completes the proof. 
\end{proof}


Next, we show how to make causal inference in the \emph{info} intervention framework, based on 
"front-door" criterion. In any graph, we say that $Z$ satisfies the front-door criterion when (i) $Z$ intercepts all directed paths from $X$ to $Y$; (ii) there are no unblocked back-door paths from $X$ to $Z$; (iii) $X$ blocks all back-door paths from $Z$ to $Y$. 
As Pearl \cite{Pearl2009}, we consider an \emph{info}-causal DAG 
with hidden variables $U$ of this front-door problem (see 
Figure \ref{fig:front2}(a)).
For this  \emph{info}-causal DAG, its 
\emph{info}-intervened DAG $G^{\info(\tilde{x})}$ is plotted in Figure \ref{fig:front2}(b).

	\begin{figure}[h]
		\begin{subfigure}[tb]{0.5\textwidth}
			\centering
			\begin{tikzpicture}
			\node[state] (x) at (0, 1) {$X$};
			\node[state] (z) at (1.2, 1)  {$Z$};
			\node[state, dashed](u) at (1, 2) 	{$U$};
			\node[state] (y) at (2.4, 1) {$Y$};
			\path[dashed] (u) edge (y);
			\path[dashed] (u) edge (x);
			\path (x) edge (z);
			\path (z) edge (y);
			\end{tikzpicture}
			\caption{An \emph{info}-causal DAG satisfying "front-door" criterion.}
			\label{fig:front1}
		\end{subfigure}
		\hfill
		\begin{subfigure}[tb]{0.5\textwidth}
			\centering
			\begin{tikzpicture}
			\node[state] (x) at (-0, 1) {$X$};
			\node[state] (z) at (1.2, 1)  {$Z$};
			\node[state, dashed](u) at (1, 2) 	{$U$};
			\node[state] (y) at (2.4, 1) {$Y$};
			\path[dashed] (u) edge (y);
			\path[dashed] (u) edge (x);
			\path (z) edge (y);
			\end{tikzpicture}
			\caption{Its \emph{info}-intervened DAG $G^{\info(\tilde{x})}$.}
			\label{fig:indep2}
		\end{subfigure}
		%	\begin{subfigure}[tb]{0.155\textwidth}
		%		\centering
		%		\begin{tikzpicture}
		%		\node[state] (x) at (0.1, 1) {$X$};
		%		\node[state] (z) at (1.1, 1)  {$Z$};
		%		\node[state, dashed](u) at (1, 2) 	{$U$};
		%		\node[state] (y) at (2, 1) {$Y$};
		%		\path[dashed] (u) edge (y);
		%		\path[dashed] (u) edge (x);
		%		\path (x) edge (z);
		%		\end{tikzpicture}
		%		\caption{$G^{\info(Z={z})}$.}
		%		\label{fig:front1}
		%	\end{subfigure}
		\caption{Two DAGs with ``front-door'' criterion.}
		\label{fig:front2}
	\end{figure}

 


\begin{Thm}[``Front-door'' criterion]
For an info-causal DAG in Figure \ref{fig:front2}, 
$p(y|\info(\tilde{x}))$ can be written as sum-product of conditional probabilities:
	\begin{equation*}
	P(y|\info(\tilde{x})) = \sum_z  P(z|\tilde{x}) \sum_x P(y|z, x) P(x).
	\end{equation*}
\end{Thm}

\begin{proof}
By (\ref{eq:sigma}), we have
	\begin{equation}
	\label{eq:front}
	P(y, z, x, u| \info(\tilde{x}) )= P(y|u,z) P(z|\tilde{x}) P(x|u) P(u).
	\end{equation}
Moreover, by Markov property, 
we can show 
\begin{equation}
	\label{eq:front:1}
P(z|u,x)=P(z|x) \,\,\,\mbox{ and }\,\,\,P(y|u,z)=P(y|u,z,x),
\end{equation}
where the first equality further implies 
\begin{equation}
	\label{eq:front:2}
P(u|z,x)=P(u|x).
\end{equation}
Hence, it follows that
	\begin{align*}
	P(y|\info(\tilde{x})) &= \sum_z \sum_x \sum_u P(y, z, x, u| \info(\tilde{x}) ) \\
	&= \sum_z \sum_x \sum_u P(y|u,z) P(z|\tilde{x}) P(x|u) P(u) \,\,\,\,\,(\mbox{by }(\ref{eq:front})) \\
	&= \sum_{z} P(z|\tilde{x}) \sum_x \sum_u P(y|u,z)  P(x|u) P(u) \\
	&= \sum_{z} P(z|\tilde{x}) \sum_x \sum_u P(y|u,z, x)  P(u|x) P(x) \,\,\,\,\,(\mbox{by }(\ref{eq:front:1}))\\
	&= \sum_{z} P(z|\tilde{x}) \sum_x P(x) \sum_u P(y|u,z, x)  P(u|x) \\
	&= \sum_{z} P(z|\tilde{x}) \sum_x P(x) \sum_u P(y|u,z, x)  P(u|z, x) \,\,\,\,\,(\mbox{by }(\ref{eq:front:2}))\\
	&= \sum_{z} P(z|\tilde{x}) \sum_x P(x)  P(y|z, x). 
	\end{align*}
This completes the proof. 	
\end{proof}


For an \emph{info}-causal DAG with confounding variables, 
one can use a  ``back-door'' criterion or more generally,  
Pearl's 3 rules. Specifically, Pearl's 3 rules for \emph{do} intervention enable us to identify a causal query and turn a causal question into a statistical estimation problem, and they describe graphical criteria for
\begin{enumerate}[1.]
	\setlength{\itemsep}{0em}
	\item insertion/deletion of observations,
	\item action/observation exchange,
	\item insertion/deletion of actions.
\end{enumerate}
Denote by $\indep_d$ the $d$-separation.
Similar to Pearl's 3 rules for $do$ intervention, we can present our three rules for \emph{info} intervention.

\begin{Thm}[Three rules for \emph{info} intervention]
	\label{thm:rules}
	For an info-causal DAG $G=(V,E)$, $X, Y, Z$ and $W$ are arbitrary disjoint sets of variables. Then, 
	%for the info intervention $\info(x)$, we have
	
	\textbf{Rule 1} (Insertion/deletion of observations) \\
	$p(y|\info(x), z, w) = p(y|\info(x), w)$ if $Y \indep_d Z|W$ in $G^{\info(x)}$;
	
	\textbf{Rule 2} (Action/observation exchange) \\
	$p(y|\info(z), x) = p(y|z, x)$  if $Y\indep_d Z|X$ in $G^{\info(z)}$;   
	
	\textbf{Rule 3} (Insertion/deletion of actions) \\
	$p(y|\info(z)) = p(y)$ if there are no causal paths from $Z$ to $Y$ in $G$.  
\end{Thm}

\begin{proof}
	In $G^{\info(x)}$, since $p(x, y, x, w|\info(x))$ factorizes, the $d$-separation implies the conditional independence.
	
	For Rule 1, we know that $Y$ and $Z$ are independent conditionally on $W$, and hence the conclusions holds.
	
	For Rule 2, since $Y$ and $Z$ are independent conditionally on $X$, we can show
	\begin{align*}
		p(y|\info(z),x) = p(y|\info(z),z,x) 
		%&= \frac{p(y, z| \info(z), x)}{p(z|\info(z), x)} \\
		= \frac{p(y, z, x |\info(z))}{p(z, x |\info(z))} 
		= \frac{p(y, z, x )}{p(z, x)},
		%&= P(Y=y|Z=z, X=x)
	\end{align*}
where the last equality holds by (\ref{a1})-(\ref{a2}). Then, the conclusion follows directly. 

	
	For Rule 3, we rephrase $X$ to be the entire random vector in $G$ for ease of presentation. 
	Let $Anc(Y)$ represent the set of ancestors of $Y$ and itself. 
	Then, by (\ref{eq:sigma}), we have 
	 \begin{align*}
	 	p(x| \sigma(z)) &= \prod_{k \in V} p(x_k|x_{pa(k)}^*) \\
	 	&= \prod_{k \in Anc(Y)} p(x_k|x_{pa(k)}^*) \cdot  \prod_{k \notin Anc(Y)} p(x_k|x_{pa(k)}^*) \\
	 	&= \prod_{k \in Anc(Y)} p(x_k|x_{pa(k)}) \cdot  \prod_{k \notin Anc(Y)} p(x_k|x_{pa(k)}^*),
	  \end{align*}
	  where we have used the fact that 
	$x^*_{pa(k)} = x_{pa(k)}$ for any $k \in Anc(Y)$, since $Z \notin Anc(Y)$.
	 Marginalizing on $Anc(Y)$, we can obtain
	\begin{align*}
		p(x_{Anc(Y)}| \sigma(z)) = \prod_{k \in Anc(Y)} p(x_k|x_{pa(k)}) 
		= p(x_{Anc(Y)}).
	\end{align*}	 	
Since $Y \in Anc(Y)$, the conclusion follows directly. This completes all of the proofs. 
\end{proof}

Similar to Theorem \ref{thm:rules}, we
can get the following results to bring us certain convenience in some applications.

%The above three rules for $\info(\cdot)$ operater have the following equivalent form, and in some case it brings some convenience for application.

\begin{Lem}
For disjoint $A, B, C, D \subset V$ in an info-causal DAG, we have

\begin{enumerate}[i)]
    \item $p(\tilde{x}|\info(\tilde{x}_A), \info(\tilde{x}_B)) = p(\tilde{x}|\info(\tilde{x}_{A\cup B})) = p(\tilde{x})$
    \item $p(x_{A\cup B}|\info(x_A)) = p(x_{A\cup B})$
    \item $p(x_A|x_B, \sigma(x_B)) = p(x_A|x_B)$
    \item $p(x_A, x_B | \sigma(x_B), \sigma(x_C)) = p(x_A, x_B |\sigma(x_C))$
    \item $p(x_A|x_B, \sigma(x_B), \sigma(x_C)) = p(x_A|x_B, \info(x_C))$
\end{enumerate}
\end{Lem}

\begin{proof}
只需要证明(iv), 它只需证明：
$$p(x_A, x_B| \sigma(x_B), \sigma(x_C)) = p(x_A, x_B|\info(x_{B\cup C})) = p(x_A, x_B |\sigma(x_C))$$
而后面的式子，我们直接用定义可以得到。
\end{proof}


\begin{Thm}
	\label{thm:rules}
	For an info-causal DAG $G=(V, E)$, $X, Y, Z$ and $W$ are arbitrary disjoint sets of variables. Then, 
	
	\textbf{Rule 1} (Insertion/deletion of observations) \\
	$p(y|\info(x), z, w) = p(y|\info(x), w)$ if $Y \indep_d Z|W$ in $G^{\info(x)}$;
	
	\textbf{Rule 2} (Action/observation exchange) \\
	$p(y|\info(x), \info(z), w) = p(y|\info(x), z, w)$ if $Y\indep_d Z|W$ in $G^{\info(x, z)}$;
	
	\textbf{Rule 3} (Insertion/deletion of actions) \\
	$p(y|\info(x), \info(z), w) = p(y|\info(x), w)$ if there are no causal paths from $Z$ to $Y$ in $G^{\info(x)}$.  
\end{Thm}

\begin{proof}
Rule 2 的证明， d-seperation 意味着独立性，所以

$$p(y|\info(x, z), w) = p(y|z, w, \info(x, z))$$

$$p(y|\info(x), \info(z), w) = p(y|z, w, \info(x), \info(z))$$

利用上面的引理(v) 可以得到我们要证明的式子。

Rule 3 的证明，类似于前面


\end{proof}





% The single-world intervention graph (SWIG) which is quite similar to info intervened graph with the only difference of additional splitted constant intervened nodes \cite{Richardson2011, richardson2013single}. It is a simple graphical theory unifying causal directed acyclic graphs (DAGs) and potential (aka counterfactual) outcomes via a node-splitting transformation, thus the three rules of causal calculus hold.





% \begin{Eg}[Independence of counterfactuals]
% 	\label{eg:info_causal_DAG}
% 	If we have a causal DAG $G$ in Figure \ref{fig:indep1}, $H$ is a hidden variable and $Y(a, b), B(a), Z(a)$ are corresponding counterfactual variables. then does the conditional independent relation $Y(a, b) \indep B(a) | \{A, Z(a)\}$ hold?
	
% 	\begin{figure}[ht]
% 		\begin{subfigure}[tb]{0.5\textwidth}
% 			\centering
% 			\begin{tikzpicture}
% 			\node[state] (a) at (-0.5, 1) {$A$};
% 			\node[state] (z) at (1, 1)  {$Z$};
% 			\node[state](b) at (1, 2) 	{$B$};
% 			\node[state, dashed] (h) at (-0.5, 2) {$H$};
% 			\node[state] (y) at (2.2, 1.5) {$Y$};
% 			\path (h) edge (z);
% 			\path (h) edge (b);
% 			\path (b) edge (y);
% 			\path (a) edge (z);
% 			\path (z) edge (b);
% 			\path (z) edge (y);
% 			\end{tikzpicture}
% 			\caption{Causal graph.}
% 			\label{fig:indep1}
% 		\end{subfigure}
% 		\hfill
% 		\begin{subfigure}[tb]{0.5\textwidth}
% 			\centering
% 			\begin{tikzpicture}
% 			\node[state] (a) at (-0.5, 1) {$A$};
% 			\node[state] (z) at (1, 1)  {$Z$};
% 			\node[state](b) at (1, 2) 	{$B$};
% 			\node[state, dashed] (h) at (-0.5, 2) {$H$};
% 			\node[state] (y) at (2.2, 1.5) {$Y$};
% 			\path (h) edge (z);
% 			\path (h) edge (b);
% 			\path (z) edge (b);
% 			%			\path (b) edge (y);
% 			%			\path (a) edge (z);
% 			\path (z) edge (y);
% 			\end{tikzpicture}
% 			\caption{Info intervened graph.}
% 			\label{fig:indep3}
% 		\end{subfigure}
% 		\caption{Independence of Counterfactuals.}
% 		\label{fig:indep}
% 	\end{figure}
% \end{Eg}

The answer is yes. The single-world intervention graph (SWIG)  \cite{Richardson2011, richardson2013single}, which is a simple graphical theory unifying causal directed acyclic graphs (DAGs) and potential (aka counterfactual) outcomes via a node-splitting transformation, is proposed for answering such questions. Here we show we are able to answer such question elegantly with info intervention and related tools.

Since the info intervened causal graph $G^{\info(A=a, B=b)}$ factorizes, we can use $d$-separation to read conditional independence from the info intervened graph Figure \ref{fig:indep3}:

$$
Y \indep_d B | \{A, Z\} \, in \, G^{\info(A=a, B=b)}
$$

which means

$$
Y^{\info(a, b)} \indep B^{\info(a, b)} | \{A^{\info(a, b)}, Z^{\info(a, b)}\} 
$$

By the definition of info casual DAG, $A^{\info(a, b)}= A$ for that $A$ doesn't recieve any information on eges $A \rightarrow Z, B \rightarrow Y$, and similarly for $B^{\info(a, b)} = B^{\info(a)}, Z^{\info(a, b)} = Z^{\info(a)}$. Then we plug them into the above conditional relation we have $Y^{\info(a, b)} \indep B^{\info(a)} | \{A, Z^{\info(a)}\}$.




%\begin{Rem}
%	Our proof method is better than Pearl's method using $do$-calculus in Eq. \ref{eq:front} for that $X$ with a non-trival distribution which faciltitates the following calculations.
%\end{Rem}


\section{Discussion and Conclusion}

%Future work including,
%
%\paragraph{Soft interventions.} Dawid proposes regime indicator to represent solf interventions which do not fix a variable at a value, but just 'nudge' it, adding a random error, or somehow shift its distribution. 
%
%
%\paragraph{Mediation analysis.} Info intervention can define s the mechanisms that transmit changes from a cause to its effects much easier. 

第一段总结，做过什么工作？
第二段展望未来。未来需要做的事情是，既然因果是信息传递，那么和信息论的更深一步结合不可避免，我们可以通过控制信息的门控来断开变量之间因果关系，这是一个解决有环因果模型的潜在途径。



We interpret causality as information transfer and propose the info intervention to highlight the information view of causal modeling. We have not only addressed the logic behind this concept, but also invented the $\info(\cdot)$ operator to formulate causal queries and causal calculus. We think $\info(\cdot)$-operator is a necessary and useful tool for causal inference complementary current used causal notations.

需要注意的事情是 input information + causal mechanism = output, $\info(\cdot)$ 干预输入信息，$do(\cdot)$ 干预的因果机制。另外后面我们可以做的事情是干预某条边上的信息。理解某个节点有5条输出边，我们只干预其中一条边上的信息。那么对应的因果图就相当于抹掉了一条边(而不是全部的输出边)，这是SWIG 的split node 做不到的事情。


It could be beneficial and applicable to the mind-brain debate to understand causality as information transfer for two reasons:  (1) the brain is an information processing machine, and (2) the way we talk about the mental to physical relationship seems similar to the relationship between the abstract content of information and its physical realization. And two challenges are: (1) if the mental is similar to the meaning of a piece of information, how can that be shown, and (2) how would causation as information transfer be able to account for the causal efficacy of the meaning of said piece of information. Or, in other words, how we could develop an explanation that avoids the challenges of causal compatibilism and interventionism, while at the same time accounts for the causal efficacy of the mental \cite{feltz2019free}. We think both $do(\cdot)$ and $\info(\cdot)$ are valuable notations to formalize three-level causal questions, and with info intervention the interventionism and information transfer understanding to causality can be unified.
